# ğŸš€ Quick Start Guide: Continued Pre-training with MLX-LM

## âœ… System Status: READY

Your continued pre-training system is fully set up and tested! Here's everything you need to know:

## ğŸ“Š Your Data

- **Documents Found**: 42 files in `mnemosyne/` directory
- **Supported Formats**: `.txt`, `.md`, `.rst`, `.py`, `.json`
- **Processing**: Recursive scanning of all subdirectories
- **Ready for Training**: âœ… Yes

## ğŸ¯ Quick Commands

### Test the System
```bash
./run_training.sh test
```

### Start Training (Recommended First Run)
```bash
# Quick 500-iteration test
./run_training.sh quick

# Full training with default model (Gemma 3-4B)
./run_training.sh

# Use different models
./run_training.sh llama     # Llama 3.1-8B
./run_training.sh mistral   # Mistral 7B
```

### Custom Training
```bash
# Direct Python usage
python continued_pretraining.py --model mlx-community/gemma-3-4b-it-bf16

# Custom parameters
python continued_pretraining.py \
    --model mlx-community/Meta-Llama-3.1-8B-Instruct-4bit \
    --batch-size 2 \
    --learning-rate 3e-6 \
    --max-iterations 5000
```

## ğŸ“‹ Your Questions Answered

### â“ Multiple Documents vs Single Document Training?
**âœ… Answer: Multiple documents in batch is significantly better**

The system automatically:
1. Processes all 42 documents in `mnemosyne/`
2. Chunks them into 2048-token segments
3. Shuffles and batches multiple chunks together
4. Trains on diverse document batches for stable gradients

### â“ When to Save the Model?
**âœ… Answer: Smart checkpointing every 500 iterations**

The system automatically:
- Saves checkpoints every 500 iterations
- Keeps the last 5 checkpoints (auto-cleanup)
- Implements early stopping when loss plateaus
- Saves final model after training completion

## ğŸ¨ Training Configurations Available

| Command | Model | Description | Time | Memory |
|---------|-------|-------------|------|--------|
| `./run_training.sh quick` | Gemma 3-4B | 500 iterations test | ~30 min | Low |
| `./run_training.sh` | Gemma 3-4B | Full training | ~4-6 hours | Medium |
| `./run_training.sh llama` | Llama 3.1-8B | Larger model | ~6-8 hours | High |
| `./run_training.sh intensive` | Gemma 3-4B | 20K iterations | ~12+ hours | Medium |
| `./run_training.sh memory-efficient` | Gemma 3-4B | Low memory usage | ~6-8 hours | Very Low |

## ğŸ“Š Expected Training Output

```
ğŸ¯ Training Configuration: Default continued pre-training
Model: mlx-community/gemma-3-4b-it-bf16
=== Preparing Training Data ===
Found 42 documents to process
Processed 156 text chunks with ~89,234 tokens
Created 140 training and 16 validation examples
=== Starting Continued Pre-training ===
Training examples: 140
Validation examples: 16
Max iterations: 10,000
Checkpoints saved every 500 iterations

Iter    500 | Loss: 3.2451 | LR: 5.00e-06 | Elapsed: 12.3m | Best: 3.2451
Iter   1000 | Loss: 2.9876 | LR: 5.00e-06 | Elapsed: 24.7m | Best: 2.9876
...
```

## ğŸ“ Output Structure

After training, you'll find:

```
models/continued_pretrained/
â”œâ”€â”€ checkpoint-500/          # Checkpoint at 500 iterations
â”œâ”€â”€ checkpoint-1000/         # Checkpoint at 1000 iterations
â”œâ”€â”€ checkpoint-1500/         # Checkpoint at 1500 iterations
â”œâ”€â”€ checkpoint-2000/         # Checkpoint at 2000 iterations
â”œâ”€â”€ checkpoint-2500/         # Checkpoint at 2500 iterations
â””â”€â”€ final/                   # Final trained model
```

## ğŸ”§ Troubleshooting

### Out of Memory?
```bash
./run_training.sh memory-efficient
# or
python continued_pretraining.py --batch-size 1 --max-seq-length 1024
```

### Want Faster Training?
```bash
python continued_pretraining.py --batch-size 8  # If you have enough memory
```

### Training Too Slow?
```bash
./run_training.sh quick  # Test with 500 iterations first
```

## ğŸ¯ Best Practices Implemented

âœ… **Batch Processing**: Multiple documents per batch for stable gradients  
âœ… **Smart Checkpointing**: Save every 500 iterations with auto-cleanup  
âœ… **Early Stopping**: Prevent overfitting with loss monitoring  
âœ… **Memory Optimization**: Gradient checkpointing and efficient chunking  
âœ… **Validation**: 10% of data held out for validation  
âœ… **Comprehensive Logging**: Full training progress tracking  

## ğŸš€ Ready to Start?

1. **Quick Test**: `./run_training.sh quick` (recommended first run)
2. **Full Training**: `./run_training.sh` 
3. **Monitor Progress**: `tail -f continued_pretraining.log`
4. **Use Trained Model**: Load from `models/continued_pretrained/`

Your system is production-ready and follows all MLX-LM best practices! ğŸ‰ 